{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b93f5a3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    " In this innovative project, we delve into the world of Natural Language Processing (NLP) with a specialized focus on flight data. Our goal is to create an interactive dialogue system that not only understands user inputs but also provides insightful responses while incorporating Part-of-Speech (POS) tagging for enhanced linguistic analysis.\n",
    "\n",
    "To achieve this, we harnessed the capabilities of libraries like NLTK, spaCy, and scikit-learn to build a robust dialogue analysis platform. The data at the core of this project comes from flight-related dialogues, enabling us to create an AI-driven system that engages users in conversations related to aviation.\n",
    "\n",
    "Our endeavor goes beyond mere text processing. By understanding grammatical constructs through POS tagging, ConvoBot+ offers a unique perspective on how language shapes communication within the flight domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe3c2e",
   "metadata": {},
   "source": [
    "# Section 1: Import Libraries\n",
    "I began by importing essential libraries, which lay the foundation for data analysis, natural language processing, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e47650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('all')\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689d432",
   "metadata": {},
   "source": [
    "# Section 2: Load and Pre-process Data\n",
    "Taking the loaded dataset, I organized it into agent and customer statements, preparing it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cfc1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"air_dialogue\")\n",
    "train_dialogues = dataset['train']['dialogue'][:1000]\n",
    "\n",
    "agent_statements_list = []\n",
    "customer_statements_list = []\n",
    "\n",
    "for dialogue in train_dialogues:\n",
    "    for statement in dialogue:\n",
    "        if statement.startswith(\"agent:\"):\n",
    "            agent_statement = statement.replace(\"agent:\", \"\").strip()\n",
    "            agent_statements_list.append(agent_statement)\n",
    "        elif statement.startswith(\"customer:\"):\n",
    "            customer_statement = statement.replace(\"customer:\", \"\").strip()\n",
    "            customer_statements_list.append(customer_statement)\n",
    "\n",
    "agent_df = pd.DataFrame({'Agent Statements': agent_statements_list})\n",
    "customer_df = pd.DataFrame({'Customer Statements': customer_statements_list})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8b359",
   "metadata": {},
   "source": [
    "# Section 3: Tokenization and Lemmatization\n",
    "To facilitate text processing, I broke down the statements into sentences and words, ensuring consistency through lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9129bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df.rename(columns={'Agent Statements': 'agent'}, inplace=True)\n",
    "customer_df.rename(columns={'Customer Statements': 'customer'}, inplace=True)\n",
    "agent_df['customer'] = customer_df['customer']\n",
    "df1=agent_df\n",
    "\n",
    "df1['agent'] = df1['agent'].str.lower()\n",
    "raw_doc = agent_df['agent']\n",
    "\n",
    "sentence_tokens = []\n",
    "for doc in raw_doc:\n",
    "    sentences = nltk.sent_tokenize(doc)\n",
    "    sentence_tokens.extend(sentences)\n",
    "\n",
    "word_tokens = []\n",
    "for sentence in sentence_tokens:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    word_tokens.extend(words)\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46401f",
   "metadata": {},
   "source": [
    "# Section 4: Token Normalization\n",
    "By removing punctuation and converting text to lowercase, I standardized tokens for a more uniform analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5916bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b68b8",
   "metadata": {},
   "source": [
    "# Section 5: Greeting Functions\n",
    "I created functions to identify and respond to user greetings, adding a touch of personalized interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7f2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = ('hello', 'hi', 'whassup', 'how are you?', 'how are you')\n",
    "greet_responses = ('hi', 'Hey', 'Hey there!', 'Hellothere!!')\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb36ea",
   "metadata": {},
   "source": [
    "# Section 6: Response Function\n",
    "With a specialized function, I generated responses by analyzing the input's TF-IDF vectorization and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763b9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robol_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "\n",
    "    if req_tfidf == 0:\n",
    "        robol_response = robol_response + \"I am sorry. Unable to understand you!\"\n",
    "        return robol_response, None\n",
    "    else:\n",
    "        robo1_response = robol_response + sentence_tokens[idx]\n",
    "        doc = nlp(robo1_response)\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        return robo1_response, pos_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03d1ac",
   "metadata": {},
   "source": [
    "# Section 7: Main Loop for Conversations\n",
    "A central loop orchestrated user-bot interactions, encompassing greetings, responses, and farewells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc07f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am the Blitz Bot.\n",
      "Choose an option:\n",
      "1. Conversation\n",
      "2. POS Tagging\n",
      "3. Exit\n",
      "Your choice: 1\n",
      "What would you like to ask?\n",
      "User: hello\n",
      "Bot: Hey\n",
      "User: my name is edward\n",
      "Bot: hello edward williams.\n",
      "User: how are you today\n",
      "Bot: what can i do for you today?\n",
      "User: i need a ticket\n",
      "Bot: to cancel your ticket, we need your name.\n",
      "User: smith\n",
      "Bot: hello smith, how can i help you for today?\n",
      "User: any flight for today\n",
      "Bot: what can i do for you today?\n",
      "User: any flight?\n",
      "Bot: no flights were found by your name.\n",
      "User: check again\n",
      "Bot: sure, i will check it.\n",
      "User: thanks\n",
      "Bot: You are welcome!\n",
      "Choose an option:\n",
      "1. Conversation\n",
      "2. POS Tagging\n",
      "3. Exit\n",
      "Your choice: 2\n",
      "Enter a sentence for POS tagging: Amazon is own by united state goverment\n",
      "POS tags: [('Amazon', 'PROPN'), ('is', 'AUX'), ('own', 'ADJ'), ('by', 'ADP'), ('united', 'PROPN'), ('state', 'PROPN'), ('goverment', 'NOUN')]\n",
      "Choose an option:\n",
      "1. Conversation\n",
      "2. POS Tagging\n",
      "3. Exit\n",
      "Your choice: 2\n",
      "Enter a sentence for POS tagging: my name is oghenefega\n",
      "POS tags: [('my', 'PRON'), ('name', 'NOUN'), ('is', 'AUX'), ('oghenefega', 'NOUN')]\n",
      "Choose an option:\n",
      "1. Conversation\n",
      "2. POS Tagging\n",
      "3. Exit\n",
      "Your choice: 3\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print('Hello, I am the Blitz Bot.')\n",
    "while True:\n",
    "    user_input = input('Choose an option:\\n1. Conversation\\n2. POS Tagging\\n3. Exit\\nYour choice: ')\n",
    "    if user_input == '1':\n",
    "        flag = True\n",
    "        print('What would you like to ask?')\n",
    "        while flag:\n",
    "            user_response = input('User: ')\n",
    "            user_response = user_response.lower()\n",
    "\n",
    "            if user_response != 'bye':\n",
    "                if user_response == 'thank you' or user_response == 'thanks':\n",
    "                    flag = False\n",
    "                    print('Bot: You are welcome!')\n",
    "                else:\n",
    "                    if greet(user_response) is not None:\n",
    "                        print('Bot: ' + greet(user_response))\n",
    "                    else:\n",
    "                        sentence_tokens.append(user_response)\n",
    "                        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                        final_words = list(set(word_tokens))\n",
    "\n",
    "                        bot_response, pos_tags = response(user_response)\n",
    "                        print('Bot:', bot_response)\n",
    "                        sentence_tokens.remove(user_response)\n",
    "            else:\n",
    "                flag = False\n",
    "                print('Bot: Goodbye!')\n",
    "                \n",
    "    elif user_input == '2':\n",
    "        sentence = input('Enter a sentence for POS tagging: ')\n",
    "        doc = nlp(sentence)\n",
    "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "        print('POS tags:', pos_tags)\n",
    "    elif user_input == '3':\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    else:\n",
    "        print('Invalid input. Please choose a valid option.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416be1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dce7f5",
   "metadata": {},
   "source": [
    "# Limitation\n",
    "One limitation of this project is that it relies on the data it has been trained on and cannot handle unseen data effectively. Since its responses are based on patterns and similarities in the provided dataset, it may struggle to provide meaningful responses to inputs that deviate significantly from the training data.\n",
    "\n",
    "Additionally, the system lacks the ability to generate entirely new text on its own. While it can select responses from the existing dataset based on similarity, it doesn't possess the creative capacity to generate original text beyond what it has been pre-programmed with. This limitation restricts its adaptability to novel or unanticipated scenarios, which could hinder its performance in dynamic conversation contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
